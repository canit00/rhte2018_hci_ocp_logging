:opencf: link:https://labs.opentlc.com/[OPENTLC lab portal^]
:account_management: link:https://www.opentlc.com/account/[OPENTLC Account Management page^]
:quay_hostname: quay.rhte.example.opentlc.com
:cluster_base: rhte-cloud1.example.opentlc.com
:cluster_master: master.{cluster_base}
:application1: app1
:application2: app2
:oc_version: 3.10.14
:oc_download_location: https://mirror.openshift.com/pub/openshift-v3/clients/{oc_version}

= Multi Cloud OpenShift CI/CD Lab

== Lab Overview

In this lab you are deploying an application to two separate OpenShift Clusters. You will complete the following tasks:

* Register for a user ID for a *Quay Enterprise* server. Quay is used as an external container image registry.
* Deploy a Jenkins server into the first OpenShift Cluster. In order to keep the system requirements relatively low our first OpenShift cluster will also act as the build environment for the application. In the real world this might be yet another OpenShift cluster that is dedicated for development teams.
* In the first OpenShift Cluster set up a Development Project and a Production Project.
* In the second OpenShift Cluster set up a Production project.
* Create a Jenkins Pipeline that
** builds a container image in the development project
* Test the Jenkins Pipeline
* Enhance and test the Jenkins Pipeline step by step to
** Push the container image to the Quay registry
** Pulls the image from Quay to the production project on both clusters
** Deploy the image to both cluster 1 and cluster 2 production projects
* Test your application from a web browser

== Lab Setup

=== Configure the OpenShift Command Line Interface

In this lab, you use the command line for some of the tasks. The lab assumes that you are using your laptop and a local OpenShift command line utility.

You will need version {oc_version} of the OpenShift command line utility. If you already have the OpenShift CLI installed you can check the version installed:

[source,bash]
----
oc version
----

.Sample Output
[source,text]
----
oc v3.10.14
kubernetes v1.10.0+b81c8f8
features: Basic-Auth
----

Make sure the the version displayed matches {oc_version}.

If it does not you can download the correct version of the OpenShift Command Line interface from {oc_download_location}. In this directory you will find a `linux`, `windows` and `macosx` subdirectory. Navigate to the directory for your operating system and download the OpenShift command line utility (oc.tar.gz for Linux and MacOS or oc.zip for Windows). You will need need to unarchive the downloaded file and then move the "oc" binary into a directory in your PATH (usually $HOME/bin).

Then verify again that the version displayed is corect.

=== Connect to the OpenShift Cluster

Make sure you can log in to the OpenShift cluster.

[NOTE]
The cluster URLs are https://{cluster_master}.

. Use the `oc login` command to log in to the cluster, making sure to replace the URLs with your cluster's URLs:
+
[source,text]
----
oc login -u <Your OPENTLC UserID> -p <Your OPENTLC Password> <CLUSTER_URL>
----
+
[NOTE]
If you see a warning about an insecure certificate, type `y` to connect insecurely.
+
.Sample Output
[source,text]
----
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>
----

== Upgrade the Logging setup

In this section you will upgrade the logging bits to logging from OpenShift version 3.11.

* SSH to the bastion host.
* Check out the repository
* Create inventory file

== Create applications in the OpenShift Cluster

In this section you are creating 2 applications in the cluster. These applications will run in separate namespaces and produce various logs. In the real world this might end-user's applications.

. Log into the OpenShift Cluster (use https://{cluster_master} as `<CLUSTER_URL>`).
+
[source,bash]
----
oc login -u <OpenTLC User Name> -p <OpenTLC Password> <CLUSTER_URL>
----
+
. Create a project for Jenkins
+
[source,bash]
----
oc new-project application1 --display-name "First Log producer"
----
+
. You will be building a (very) simple Node.JS application. OpenShift comes with a pre-configured Node.JS slave pod for Jenkins to use. However you will be moving images between various container registries using *skopeo* - which is not included in the stock slave pod. Therefore you will need to build your own custom slave pod based on the stock Node.JS pod to include skopeo.
+
You can use OpenShift to build this custom slave pod by executing a simple Docker build. Create a Build Configuration to build a custom NodeJS Slave Pod that includes *skopeo*. The container image for this new slave pod will live in your Jenkins project as *jenkins-slave-nodejs-skopeo*. This build will automatically start and succeed after a few seconds.
+
[source,bash]
----
oc new-build -D $'FROM docker.io/openshift/jenkins-slave-nodejs-centos7:v3.9\nUSER root\nRUN yum -y install skopeo && yum clean all\nUSER 1001' --name=jenkins-slave-nodejs-skopeo --to=jenkins-slave-nodejs-skopeo:v3.9 -n ${GUID}-jenkins
----

== Create projects in OpenShift Cluster 1

In this section you are creating a development project in the first OpenShift Cluster and a production project in both the first and second OpenShift Cluster. Again in the real world you would most likely have three clusters, one for development and two separate production clusters. For simplicity you are using the first production cluster to also build your application.

One important part of setting up the OpenShift projects is to grant the correct permissions to Jenkins to build and deploy the application.

. Make sure to be still logged into OpenShift Cluster 1 (https://{cluster_master})
+
[source,bash]
----
oc login -u <Your OpenTLC User ID> <CLUSTER_URL>
----

. Create the development project
+
[source,bash]
----
oc new-project ${GUID}-rhte-app-dev --display-name "RHTE Application (Development)"
----
+
. Grant the correct permissions to Jenkins - the *jenkins* service account needs *edit* permissions to manipulate the projects.
+
[source,bash]
----
oc policy add-role-to-user edit system:serviceaccount:${GUID}-jenkins:jenkins -n ${GUID}-rhte-app-dev
----
+
. Because the Jenkins Node.JS Slave pod will be building our application you will need to set up a binary build configuration in your development project. Jenkins will simply "stream" the built artifacts into this build configuration to create the runtime container image.
+
[source,bash]
----
oc new-build --binary=true --name="rhte-app" nodejs:8 -n ${GUID}-rhte-app-dev
----

. Create the production project
+
[source,bash]
----
oc new-project ${GUID}-rhte-app --display-name "RHTE Application"
----
+
. Again grant the correct permissions to Jenkins
+
[source,bash]
----
oc policy add-role-to-user edit system:serviceaccount:${GUID}-jenkins:jenkins -n ${GUID}-rhte-app
----

. Create the production deployment config and set it up for deployment from Jenkins. Note the following:
* You are using *nodejs:0.0* as the image to be deployed - this image does not exist. Therefore you need to specify the *--allow-missing-imagestream-tags=true* parameter. The pipeline will build the image, make it available to this project and then update the deployment configuration with the correct image.
* You are turning off all triggers for the deployment configuration - both image change and config triggers. This allows the Jenkins Pipeline to fully control the deployment of the application.
* You are setting two environment variables for the deployment config: *CLUSTER_NAME* and *PREFIX*. The Node.JS application expects these variable to be set to display which cluster it is running on.
* Since there is no container image to introspect for OpenShift you have to manually create the service for the application. The Node.JS application listens on port 3000 - therefore create a service to point to port 3000.
+
[source,bash]
----
oc new-app openshift/nodejs:0.0 --name=rhte-app --allow-missing-imagestream-tags=true -n ${GUID}-rhte-app
oc set triggers dc/rhte-app --remove-all -n ${GUID}-rhte-app
oc set env dc/rhte-app CLUSTER_NAME="Cluster 1" -n ${GUID}-rhte-app
oc set env dc/rhte-app PREFIX="${GUID}" -n ${GUID}-rhte-app
oc expose dc rhte-app --port 3000 -n ${GUID}-rhte-app
----
+
. To make the application available from outside the OpenShift cluster you need to expose the service as a route. But since you are deploying the application to two clusters there is also a load balancer in play - which means that you need to create a second route for the load balancer URL.
+
The first is the standard route so that you can verify on cluster 1 that your application is working. The second route listens to forward requests from the load balancer in front of our two clusters. The load balancer is listening on *apps.rhte.example.opentlc.com* and forwards requests in a round-robin fashion to both clusters. In order for the OpenShift router to know which application to route requests to *apps.rhte.example.opentlc.com* to it needs to have a route registered for this URL.
+
[source,bash]
----
oc expose svc/rhte-app --name rhte-app -n ${GUID}-rhte-app
oc expose svc/rhte-app --name rhte-app-lb --hostname=${GUID}.apps.rhte.example.opentlc.com -n ${GUID}-rhte-app
----
+
. Create a service account for Jenkins to use to manipulate images and deployment configurations in this project.
+
[source,bash]
----
oc create sa jenkinsaccess -n ${GUID}-rhte-app
----
+
. Jenkins needs permission to copy the image to the production project as well as manipulate deployments. Therefore assign the `edit` role to this new service account.
+
[source,bash]
----
oc policy add-role-to-user edit system:serviceaccount:${GUID}-rhte-app:jenkinsaccess -n ${GUID}-rhte-app
----
+
. Retrieve the token for the *jenkinsaccess* service account. You will use this token to push the image from Quay into the production project from the Jenkins Pipeline as well as deploying the application.
+
[source,bash]
----
oc serviceaccounts get-token jenkinsaccess -n ${GUID}-rhte-app
----
+
.Sample Output
[source,text]
----
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJ4eXotcmh0ZS1hcHAiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiamVua2luc2FjY2Vzcy10b2tlbi1zeGJtMiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJqZW5raW5zYWNjZXNzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYTk0M2Q0MDktODU1MS0xMWU4LThiZmEtMGFiOWNlM2Q5ZGI4Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Onh5ei1yaHRlLWFwcDpqZW5raW5zYWNjZXNzIn0.hiwvcZqw__4Mw_gNRdoY63qaW8jvPqD-7_M0bxd98h8qSvYO4IEwBlJNLabR3SsQlKEB9m4-B2OsA_e5P_UwfTil4E23zOcaUXTTwjV06RuaxHvI0v3POb_4_hWql_JmDr0QlyWBuFMtFBNvhto32y0Yeyn5E8boh8u33RfmR_p5Kq029UXtBZAAp-7iHWiszK7NUOZ4PpzV7NXJCj9Toj8_4oNSzAuo7Uqv5q_KX-IyzPwuFU-krG9V4tkmM9QaeAZxmpck-tCSQztm5H7ssdrf8k_ImHG2OsKJMVgY3vPEZROv1JXyY01xxUsB03Rti9wSUnscJgM7jaZ6q-qwjA
----
+
. Save this token somewhere (like a temporary text file). You will need it when setting up the pipeline.

== Create projects in OpenShift Cluster 2

In the second OpenShift Cluster you will create just a Production project. In the Jenkins Pipeline you will be copying the container image from your development project in the first cluster to Quay and then from Quay to the production projects in your two clusters. This guarantees that both clusters are using the same container image.

. Log into the second OpenShift Cluster (https://{cluster2_master})
+
[source,bash]
----
oc login -u <OpenTLC User ID> <CLUSTER_URL>
----
+
. Create the production project
+
[source,bash]
----
oc new-project ${GUID}-rhte-app --display-name "RHTE Application"
----
+
. Just like in the first cluster create the production deployment config and set it up for deployment from Jenkins
+
[source,bash]
----
oc new-app openshift/nodejs:0.0 --name=rhte-app --allow-missing-imagestream-tags=true -n ${GUID}-rhte-app
oc set triggers dc/rhte-app --remove-all -n ${GUID}-rhte-app
oc set env dc/rhte-app CLUSTER_NAME="Cluster 2" -n ${GUID}-rhte-app
oc set env dc/rhte-app PREFIX="${GUID}" -n ${GUID}-rhte-app
oc expose dc rhte-app --port 3000 -n ${GUID}-rhte-app
----
+
. Again create two routes for the application. The first is the standard route so that you can verify on cluster 2 that your application is working. The second route listens to forward requests from the load balancer in front of our two clusters.
+
[source,bash]
----
oc expose svc/rhte-app --name rhte-app -n ${GUID}-rhte-app
oc expose svc/rhte-app --name rhte-app-lb --hostname=${GUID}.apps.rhte.example.opentlc.com -n ${GUID}-rhte-app
----
+
[TIP]
Both production projects have a route with hostname *${GUID}.apps.rhte.example.opentlc.com*. This enables the external loadbalancer that is receiving traffic at that URL to forward requests to either cluster. And the router on each cluster then knows which application to route the request to.
+
. Create a service account for Jenkins to use to manipulate images and deployment configurations in this project.
+
[source,bash]
----
oc create sa jenkinsaccess -n ${GUID}-rhte-app
----
+
. Jenkins needs permission to copy the image to the production project as well as manipulate deployments. Therefore assign the `edit` role to this new service account.
+
[source,bash]
----
oc policy add-role-to-user edit system:serviceaccount:${GUID}-rhte-app:jenkinsaccess -n ${GUID}-rhte-app
----
+
. Retrieve the token for the *jenkinsaccess* service account. You will use this token to push the image from Quay into the production project from the Jenkins Pipeline.
+
[source,bash]
----
oc serviceaccounts get-token jenkinsaccess -n ${GUID}-rhte-app
----
+
.Sample Output
[source,text]
----
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJ4eXotcmh0ZS1hcHAiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiamVua2luc2FjY2Vzcy10b2tlbi1kNHh4ZiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJqZW5raW5zYWNjZXNzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMDc1NGQyYTgtODU1NS0xMWU4LWExODctMDZkYjM0OGJkODA2Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Onh5ei1yaHRlLWFwcDpqZW5raW5zYWNjZXNzIn0.HKTmmvaTwgYi8hbgKwvtbiCBAKVVU5T7Gq7HO8_qmjGfDhWMB2oP6pPExoyzN1t3615RHFBXNfJLAri2t4qurmM1n0aHxrc8kMiFXMsoEPkoaCMawbQ0W-JwEF9Y-DRdszmDbP2tPr230hQBTeC1mPT6m1H3f5nGtxJL1aAIzF8wsZwtKx4ELaSIpMO-zZCPB1EQ7NyiZ3jIKB-JZ0FnJlnwH8UF4Z8jFdgghLYfYqWSNpwDbLpYjuRU98hkugSbjfceT-7TJL1uZU_dcR04kZf_4cNT05AJ_p2jkc6NCUdK-yhRb3FCxK1Tupmp9n2uBGUojDiAcqxhon2Ed8JjxQ
----
+
. Save this token somewhere (like a temporary text file). You will need it when setting up the pipeline.

. This concludes the setup section.

== Create a Jenkins Pipeline

In this section you are creating a Jenkins Pipeline in OpenShift Cluster 1 that will build the container image from source and then deploy it to both production projects in both OpenShift Clusters.

[NOTE]
Due to time constraints in this lab you are using a rather simplified pipeline. Regardless the most important concepts can be demonstrated very well using this pipeline.

. Log back into Cluster 1 (https://{cluster_master})
. By the time you reach this step Jenkins should be ready. Make sure that your Jenkins pod is fully up and running:
+
[source,bash]
----
oc get pod -n ${GUID}-jenkins
----
+
.Sample Output
[source,text]
----
NAME                                  READY     STATUS      RESTARTS   AGE
jenkins-2-qz4qs                       1/1       Running     0          22m
jenkins-slave-nodejs-skopeo-1-build   0/1       Completed   0          22m
----
+
. Make sure that the column under *READY* reads *1/1* next to your running Jenkins Pod (like in the example output above) before proceeding. You should also see that the build pod that built your custom Node.JS slave image shows as *Completed*.
. Retrieve the route for your Jenkins instance
+
[source,bash]
----
oc get route -n ${GUID}-jenkins
----
+
.Sample Output
[source,text, options=nowrap]
----
NAME      HOST/PORT                                                   PATH  SERVICES   PORT      TERMINATION     WILDCARD
jenkins   jenkins-xyz123-jenkins.apps.rhte-cloud1.example.opentlc.com       jenkins    <all>     edge/Redirect   None
----
+
. Using the route you just retrieved (*jenkins-xyz123-jenkins.apps.rhte-cloud1.example.opentlc.com* in the example above) open your web browser and navigate to your Jenkins Instance.
. It may be necessary to confirm a security exception.
. Click *Log in with OpenShift* and use your OpenTLC credentials on the login screen.
. On the following *Authorize Access* screen click *Allow selected Permissions*.
. After a while you will see the Jenkins Homepage.
. Click `New Item` in the top left corner of the Jenkins Homepage
.. Use the following values:
* Item Name: *RHTE App*
* Select *Pipeline*
.. Click *OK* at the bottom of the page.
. Find the checkbox next to *This project is parametrized* and check it.
. Add 5 Parameters by clicking *Add Parameter*, selecting the type of parameter and entering both *Name* and *Default Values*
+
|====
|NAME|TYPE|DEFAULT
|cluster_TOKEN|String|The token for the *jenkinsaccess* service account you retrieved earlier from Cluster 1
|CLUSTER2_TOKEN|String|The token for the *jenkinsaccess* service account you retrieved earlier from Cluster 2
|QUAY_USER|String|Your user name for Quay (*NOT* the name of your Robot Account)
|QUAY_TOKEN|Password|Your robot account token for quay that you saved when you created your Quay account
|PREFIX|String|Your shortname prefix (e.g `xyz123`) for all your project names.
|====
+
. Scroll down and copy/paste the following pipeline into the Pipeline Box
+
[source,groovy]
----
// Set up variables
def quay_url      = "quay.rhte.example.opentlc.com"
def cluster_base = "rhte-cloud1.example.opentlc.com"
def cluster2_base = "rhte-cloud2.example.opentlc.com"
def prodTag       = "1.${BUILD_NUMBER}"

pipeline {
  agent {
    // Run the pipeline on a pod in Kubernetes/OpenShift
    kubernetes {
      // Define the slave pod based on our custom slave pod container image
      label "skopeo-pod"
      cloud "openshift"
      inheritFrom "nodejs"
      containerTemplate {
        name "jnlp"
        image "docker-registry.default.svc:5000/${PREFIX}-jenkins/jenkins-slave-nodejs-skopeo:v3.9"
        resourceRequestMemory "1Gi"
        resourceLimitMemory "2Gi"
        resourceRequestCpu "500m"
        resourceLimitCpu "1"
      }
    }
  }
  stages {
    stage("Checkout Source Code") {
      steps {
        echo "Checking out Source Code"
        git 'https://github.com/wkulhanek/rhte-app.git'
      }
    }
    stage("Build Application") {
      steps {
        echo "Building Application"

        // This is a Node.JS application and the slave pod has everything necessary
        // to build this application.
        sh "source /opt/rh/rh-nodejs4/enable && npm install"
      }
    }
    stage("Build Container Image") {
      steps {
        script {
          echo "Building Container Image"

          // Use the cluster that Jenkins is running in
          openshift.withCluster() {

            // Use the "${PREFIX}-rhte-app-dev" project on that cluster
            openshift.withProject("${PREFIX}-rhte-app-dev") {
              // Use OpenShift to create the container image. Start the build for build configuration
              // 'rhte-app' - which you created before as a binary build configuration. This means that
              // Jenkins just copies the contents of the working directory ('.') into the builder image
              // rather than the builder image doing the 'npm install'.
              openshift.selector("bc", "rhte-app").startBuild("--from-dir=.", "--wait=true")

              // Tag the built image as "rhte-app:${prodTag}"
              openshift.tag("rhte-app:latest", "rhte-app:${prodTag}")
            }
          }
        }
      }
    }

    // ----------------------------------
    // Copy to Quay BEGIN

    // Copy to Quay END
    // ----------------------------------

    // ----------------------------------
    // Copy to Clusters BEGIN

    // Copy to Clusters END
    // ----------------------------------

    // ----------------------------------
    // Deploy Images BEGIN

    // Deploy Images END
    // ----------------------------------
  }
}
----
+
This pipeline is written in *Declarative* Jenkinsfile syntax. Read through the pipeline and try to understand what the steps in the pipeline accomplish. This pipeline so far checks out source code, builds the Node.JS application (which is really just `npm install` and then uses a binary build to create the container image in the development project.
+
. Click *Save*

. Click *Build with Parameters* on the left hand side
* This opens the screen where you can enter parameters. If you set the default values as suggested above you do not have to edit anything there.
* Click *Build*
. You can follow along as the pipeline builds by clicking on the little triangle next to the *build number* in the *Build History* panel and selecting *Console Output*.
+
A successful run will have console output similar to the one below:
+
.Sample Output
[source,text]
----
Started by user wkulhane-redhat.com
[Pipeline] podTemplate
[Pipeline] {
[Pipeline] node
Still waiting to schedule task
Jenkins doesn’t have label skopeo-pod
Running on skopeo-pod-nj8pc-pppfz in /home/jenkins/workspace/RHTE App
[Pipeline] {
[Pipeline] container
[Pipeline] {
[Pipeline] stage
[Pipeline] { (Checkout Source Code)
[Pipeline] echo
Checking out Source Code
[Pipeline] git
Cloning the remote Git repository
Cloning repository https://github.com/wkulhanek/rhte-app.git
 > git init /home/jenkins/workspace/RHTE App # timeout=10
Fetching upstream changes from https://github.com/wkulhanek/rhte-app.git
 > git --version # timeout=10
 > git fetch --tags --progress https://github.com/wkulhanek/rhte-app.git +refs/heads/*:refs/remotes/origin/*
 > git config remote.origin.url https://github.com/wkulhanek/rhte-app.git # timeout=10
 > git config --add remote.origin.fetch +refs/heads/*:refs/remotes/origin/* # timeout=10
 > git config remote.origin.url https://github.com/wkulhanek/rhte-app.git # timeout=10
Fetching upstream changes from https://github.com/wkulhanek/rhte-app.git
 > git fetch --tags --progress https://github.com/wkulhanek/rhte-app.git +refs/heads/*:refs/remotes/origin/*
 > git rev-parse refs/remotes/origin/master^{commit} # timeout=10
 > git rev-parse refs/remotes/origin/origin/master^{commit} # timeout=10
Checking out Revision 44246d444181a3872e740883f0ebd69b8c818830 (refs/remotes/origin/master)
 > git config core.sparsecheckout # timeout=10
 > git checkout -f 44246d444181a3872e740883f0ebd69b8c818830
 > git branch -a -v --no-abbrev # timeout=10
 > git checkout -b master 44246d444181a3872e740883f0ebd69b8c818830
Commit message: "Added output of PREFIX environment"
First time build. Skipping changelog.
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Build Application)
[Pipeline] echo
Building Application
[Pipeline] sh
[RHTE App] Running shell script
+ source /opt/rh/rh-nodejs4/enable
++ export PATH=/opt/rh/rh-nodejs4/root/usr/bin:/home/jenkins/node_modules/.bin/:/home/jenkins/.npm-global/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/rh/rh-nodejs4/root/usr/bin:/home/jenkins/node_modules/.bin/:/home/jenkins/.npm-global/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export LD_LIBRARY_PATH=/opt/rh/rh-nodejs4/root/usr/lib64
++ LD_LIBRARY_PATH=/opt/rh/rh-nodejs4/root/usr/lib64
++ export PYTHONPATH=/opt/rh/rh-nodejs4/root/usr/lib/python2.7/site-packages
++ PYTHONPATH=/opt/rh/rh-nodejs4/root/usr/lib/python2.7/site-packages
++ export MANPATH=/opt/rh/rh-nodejs4/root/usr/share/man:
++ MANPATH=/opt/rh/rh-nodejs4/root/usr/share/man:
+ npm install
EXITCODE   0cookie-parser@1.4.3 node_modules/cookie-parser
├── cookie-signature@1.0.6
└── cookie@0.3.1

http-errors@1.6.3 node_modules/http-errors
├── setprototypeof@1.1.0
├── inherits@2.0.3
├── statuses@1.5.0
└── depd@1.1.2

morgan@1.9.0 node_modules/morgan
├── on-headers@1.0.1
├── depd@1.1.2
├── basic-auth@2.0.0 (safe-buffer@5.1.1)
└── on-finished@2.3.0 (ee-first@1.1.1)

debug@2.6.9 node_modules/debug
└── ms@2.0.0

express@4.16.3 node_modules/express
├── escape-html@1.0.3
├── array-flatten@1.1.1
├── setprototypeof@1.1.0
├── cookie-signature@1.0.6
├── utils-merge@1.0.1
├── content-type@1.0.4
├── methods@1.1.2
├── merge-descriptors@1.0.1
├── encodeurl@1.0.2
├── etag@1.8.1
├── range-parser@1.2.0
├── cookie@0.3.1
├── serve-static@1.13.2
├── path-to-regexp@0.1.7
├── parseurl@1.3.2
├── fresh@0.5.2
├── vary@1.1.2
├── content-disposition@0.5.2
├── statuses@1.4.0
├── safe-buffer@5.1.1
├── depd@1.1.2
├── qs@6.5.1
├── on-finished@2.3.0 (ee-first@1.1.1)
├── finalhandler@1.1.1 (unpipe@1.0.0)
├── proxy-addr@2.0.4 (forwarded@0.1.2, ipaddr.js@1.8.0)
├── send@0.16.2 (ms@2.0.0, destroy@1.0.4, mime@1.4.1)
├── type-is@1.6.16 (media-typer@0.3.0, mime-types@2.1.19)
├── accepts@1.3.5 (negotiator@0.6.1, mime-types@2.1.19)
└── body-parser@1.18.2 (bytes@3.0.0, iconv-lite@0.4.19, raw-body@2.3.2)

pug@2.0.0-beta11 node_modules/pug
├── pug-runtime@2.0.4
├── pug-strip-comments@1.0.3 (pug-error@1.3.2)
├── pug-load@2.0.11 (object-assign@4.1.1, pug-walk@1.1.7)
├── pug-linker@2.0.3 (pug-error@1.3.2, pug-walk@1.1.7)
├── pug-parser@2.0.2 (pug-error@1.3.2, token-stream@0.0.1)
├── pug-lexer@3.1.0 (pug-error@1.3.2, is-expression@3.0.0, character-parser@2.2.0)
├── pug-filters@2.1.5 (pug-error@1.3.2, pug-walk@1.1.7, resolve@1.8.1, jstransformer@1.0.0, clean-css@3.4.28, uglify-js@2.8.29, constantinople@3.1.2)
└── pug-code-gen@1.1.1 (pug-error@1.3.2, pug-attrs@2.0.3, js-stringify@1.0.2, doctypes@1.1.0, void-elements@2.0.1, with@5.1.1, constantinople@3.1.2)
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Build Container Image)
[Pipeline] script
[Pipeline] {
[Pipeline] echo
Building Container Image
[Pipeline] echo

[Pipeline] _OcContextInit
[Pipeline] _OcContextInit
[Pipeline] readFile
[Pipeline] _OcAction
[Pipeline] readFile
[Pipeline] _OcAction
[Pipeline] }
[Pipeline] // script
[Pipeline] }
[Pipeline] // stage
[Pipeline] }
[Pipeline] // container
[Pipeline] }
[Pipeline] // node
[Pipeline] }
[Pipeline] // podTemplate
[Pipeline] End of Pipeline
Finished: SUCCESS
----


== Enhance the Jenkins Pipeline to copy the built image to Quay

Now that you can build a container image successfully you could deploy it in the development project and run all kinds of tests.  Between the build and the creation of the image you could have also run unit tests stages. In this lab we don't really have time for any tests thereore you immediately created the container image.

The next step is to make this container image available to all production clusters. Previously you set up a Quay User ID and robot account. And you added *skopeo* to the Node.JS slave builder pod to enable copying of container images.

. Open your Pipeline configuration again by clicking *Configure* (on the left) on the Pipeline Item.
. Add this code to the pipeline between the markers `// Copy to Quay BEGIN` and `// Copy to Quay END`:
+
[source,groovy]
----
    stage("Copy Image to Quay") {
      steps {
        script {
          // The container image is in the development project.
          // skopeo needs permission to access this image
          // The jenkins service account that this pod is running under does have the correct permissions because you set this up in the
          // Development project earlier.
          // Retrieve the token for the Jenkins Service Account
          def jenkinsToken = sh(returnStdout: true, script: "oc whoami -t").trim()

          // Now use skopeo to copy the image from the integrated
          // container registry to Quay. Both registries do
          // not have proper certificates and will need
          // --*-tls-verify=false set. Note how you are using the Quay robot account
          // to access the Quay registry.
          sh "skopeo copy --src-tls-verify=false --dest-tls-verify=false --src-creds openshift:${jenkinsToken} --dest-creds ${QUAY_USER}+jenkins:${QUAY_TOKEN} docker://docker-registry-default.apps.${cluster_base}/${PREFIX}-rhte-app-dev/rhte-app:${prodTag} docker://${quay_url}/${QUAY_USER}/rhte-app:${prodTag}"
        }
      }
    }
----
+
. Save the updated pipeline and build it again.
. If you inspect the console output of the build you will see logs at the end similar to the example output below.
+
.Sample Output
[source,text]
----
[...]

[Pipeline] stage
[Pipeline] { (Build Container Image)
[Pipeline] script
[Pipeline] {
[Pipeline] echo
Building Container Image
[Pipeline] echo

[Pipeline] _OcContextInit
[Pipeline] _OcContextInit
[Pipeline] readFile
[Pipeline] _OcAction
[Pipeline] readFile
[Pipeline] _OcAction
[Pipeline] }
[Pipeline] // script
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Copy Image to Quay)
[Pipeline] script
[Pipeline] {
[Pipeline] sh
[RHTE App] Running shell script
+ oc whoami -t
[Pipeline] sh
[RHTE App] Running shell script
+ skopeo copy --src-tls-verify=false --dest-tls-verify=false --src-creds openshift:eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJ3a3VsaGFuZS1qZW5raW5zIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImplbmtpbnMtdG9rZW4ta3NwanYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiamVua2lucyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImQ2MDA3MWRlLTliZjktMTFlOC04YjM0LTAyMThhZmVhZDFjZSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDp3a3VsaGFuZS1qZW5raW5zOmplbmtpbnMifQ.YdKCiokG2vxEznCW2CMV60J92tKAVaRXNP2VfxZmhDIdWOs3NuZYxqjg-L-7dr5_Z-Jdpps2dGuUfKQASiAhIv9wLlJbQdHi58Djx0CbEtqMm9hF9J6scxGjhkYs3Ik3iLRMcoQQvqH5GBFLdwYJN7UEjUFGuRyULBNC0bzTFhnNPoQf6krqk3gF6EHf_HoXZDoklMRQEL2O4kzDep6pEcwE-MP6p760-lJYIF10qBk1crk9aQOG2BMQsm7DW-vR-wz8pIzpoGn9B7YP7oXMg4g84dP9u5qu4wc6TYv6cu-W3rYmzAQQbAPJTuEIhdIzTgjZ6us0hUNUPeRz7_i2hQ --dest-creds wkulhane+jenkins:77TU1T1RUSI9I1VCCR8M6Y7SF5MLJZTQBY4PFCA74TSQRW7EM2NXG8IFDQ2F1K7Z docker://docker-registry-default.apps.rhte-cloud1.example.opentlc.com/wkulhane-rhte-app-dev/rhte-app:1.2 docker://quay.rhte.example.opentlc.com/wkulhane/rhte-app:1.2
Getting image source signatures
Copying blob sha256:428a9ca37f0e50a5d54a0540773f2c7d402b27ed1264cfef3b7b8d87ee102ec8
 47.60 MB / 71.45 MB
 53.22 MB / 71.45 MB
 59.12 MB / 71.45 MB
 65.21 MB / 71.45 MB
 71.02 MB / 71.45 MB
 71.45 MB / 71.45 MB  2s
Copying blob sha256:8115a58d83bdb789829910c62fc48223d5204954608a08f612f06bdd162677ab
 0 B / 1.27 KB
 1.27 KB / 1.27 KB  0s
Copying blob sha256:339bc4cdfd0f5fb6f1efb3efc66de4968af86634e1d852bfdb68a74ac66d5a39

 0 B / 7.30 MB
 3.48 MB / 7.30 MB
 7.30 MB / 7.30 MB
 7.30 MB / 7.30 MB  0s
Copying blob sha256:a1a7c4c6afd5e6c93096132070c2f2307a57131c0ac672ac70a684475d316fe9
 65.40 MB / 79.81 MB
 71.14 MB / 79.81 MB
 76.82 MB / 79.81 MB
 79.81 MB / 79.81 MB
 79.81 MB / 79.81 MB  2s
Copying blob sha256:a41ced1af98ed6be4aff96df12e8c7a53f57961d40a69e68fdd2a59485a25795
 4.07 MB / 7.96 MB
 7.96 MB / 7.96 MB
 7.96 MB / 7.96 MB  0s
Copying blob sha256:488a08865807161c63d05aefaebd24a5332fbb3ed17bb904863784f0eda6fa8b

 0 B / 3.97 MB
 3.31 MB / 3.97 MB
 3.97 MB / 3.97 MB  0s
Copying config sha256:4f1a7bade48b9c0ee0cfe91f9bf2f0f9b4035aba58a1ac6b2e1e666c980b2a98

 0 B / 8.51 KB
 8.51 KB / 8.51 KB  0s
Writing manifest to image destination
Writing manifest to image destination
Storing signatures
[Pipeline] }

[...]
----
+
. Once the pipeline finishes successfully log back into Quay (http://{quay_hostname}) and validate that your image is now available in Quay:
* Click the repository (*<shortname>/rhte-app*) to open the repository details.
* Click on the *Tags* icon (second from the top on the left) to display all tags for this repository. You should see the tag number that corresponds to the pipeline build number.
* You will see a timestamp when the image was *Last Modified* next to your repository name. This timestamp should be close to the current time.
. Feel free to run the updated pipeline one more time and verify that that image is available in Quay as well - as a new tag.

== Enhance the Jenkins Pipeline to copy the built image from Quay to the Production Clusters

Now that the image is in Quay the pipeline can copy it to both production clusters. Once again you are using *skopeo* to copy the image from Quay to the projects. An alternative would be to configure the clusters to allow image pulling from the Quay registry - but our clusters do not have that configuration set.

. Open your Pipeline configuration again by clicking *Configure* (on the left) on the Pipeline Item.
. Add this code to the pipeline between the markers `// Copy to Clusters BEGIN` and `// Copy to Clusters END`:
+
[source,groovy]
----
    stage("Copy image to Production Cluster(s)") {
      steps {
        // Copy the image from Quay to Cluster 1
        // You are using the token for the serviceaccount `jenkinsaccess`
        // that you created in the production account earlier.
        sh "skopeo copy --src-tls-verify=false --dest-tls-verify=false --dest-creds openshift:${cluster_TOKEN} --src-creds ${QUAY_USER}+jenkins:${QUAY_TOKEN} docker://${quay_url}/${QUAY_USER}/rhte-app:${prodTag}  docker://docker-registry-default.apps.${cluster_base}/${PREFIX}-rhte-app/rhte-app:${prodTag}"

        // Same for Cluster 2
        sh "skopeo copy --src-tls-verify=false --dest-tls-verify=false --dest-creds openshift:${CLUSTER2_TOKEN} --src-creds ${QUAY_USER}+jenkins:${QUAY_TOKEN} docker://${quay_url}/${QUAY_USER}/rhte-app:${prodTag}  docker://docker-registry-default.apps.${cluster2_base}/${PREFIX}-rhte-app/rhte-app:${prodTag}"
      }
    }
----
+
. Save the updated pipeline and build it again.
. You will see in the build logs that the image has been copied successfully from Quay to both clusters.
+
.Sample Output
[source,text]
----
[...]

[Pipeline] stage
[Pipeline] { (Copy image to Production Cluster(s))
[Pipeline] sh
[RHTE App] Running shell script
+ skopeo copy --src-tls-verify=false --dest-tls-verify=false --dest-creds openshift:eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJ3a3VsaGFuZS1yaHRlLWFwcCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJqZW5raW5zYWNjZXNzLXRva2VuLWI2Nnd2Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImplbmtpbnNhY2Nlc3MiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhNmMxZjBmYS05YmZhLTExZTgtOGIzNC0wMjE4YWZlYWQxY2UiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6d2t1bGhhbmUtcmh0ZS1hcHA6amVua2luc2FjY2VzcyJ9.WbaDFfPI0A097_w_RuPG8NxkIp9_ZtO4CJK5-BAK-ew7NOcosRT7SPvN-M2XUXe12cdAChA761hpzDTCH2sj0vsZH82TngD_wJ_nCmvsr6hm3gQ_sZAgLIJj0wVFtqg3fV7S5CMcWn8QQnbA3AKbnePKUvMvLcpmL3lBKSG2gOftEi6ThN-UZzcCBJaN-uR4bV_UG499HPiuLYUKKvplsujag0vTLbobcSU69gLfVz-9z1vIwH4B2e_8zqH2c55NOtywfni9Rw9Gf7MouHilHfptGqXFBEbA85nVOymOI540xzH-CDdhsR_L19qP41fD2zGiwU6-FbUvICLkDtxhbw --src-creds wkulhane+jenkins:77TU1T1RUSI9I1VCCR8M6Y7SF5MLJZTQBY4PFCA74TSQRW7EM2NXG8IFDQ2F1K7Z docker://quay.rhte.example.opentlc.com/wkulhane/rhte-app:1.4 docker://docker-registry-default.apps.rhte-cloud1.example.opentlc.com/wkulhane-rhte-app/rhte-app:1.4
Getting image source signatures
Copying blob sha256:428a9ca37f0e50a5d54a0540773f2c7d402b27ed1264cfef3b7b8d87ee102ec8
 71.45 MB / ?
 71.45 MB / ?  5s
Copying blob sha256:8115a58d83bdb789829910c62fc48223d5204954608a08f612f06bdd162677ab

 0 B / ?
 1.27 KB / ?
 1.27 KB / ?  0s
Copying blob sha256:339bc4cdfd0f5fb6f1efb3efc66de4968af86634e1d852bfdb68a74ac66d5a39
 0 B / ?
 2.22 MB / ?
 5.73 MB / ?
 7.30 MB / ?
 7.30 MB / ?  0s
Copying blob sha256:a1a7c4c6afd5e6c93096132070c2f2307a57131c0ac672ac70a684475d316fe9
 79.81 MB / ?
 79.81 MB / ?  6s
Copying blob sha256:a41ced1af98ed6be4aff96df12e8c7a53f57961d40a69e68fdd2a59485a25795
 7.96 MB / ?
 7.96 MB / ?  1s
Copying blob sha256:4d2c6556481974e9ac9519f75ebdc77b9d3359f66cfceec8774d1512eb8df1da
 3.97 MB / ?
 3.97 MB / ?  0s
Writing manifest to image destination
Storing signatures
[Pipeline] sh
[RHTE App] Running shell script
+ skopeo copy --src-tls-verify=false --dest-tls-verify=false --dest-creds openshift:eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJ3a3VsaGFuZS1yaHRlLWFwcCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJqZW5raW5zYWNjZXNzLXRva2VuLWZ3bXNtIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImplbmtpbnNhY2Nlc3MiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyNDgzZTkxMy05YmZiLTExZTgtYmZjMy0wNjAxYmI3MDkzNjAiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6d2t1bGhhbmUtcmh0ZS1hcHA6amVua2luc2FjY2VzcyJ9.kG-fkZKnGWSEtMHEdOyrKRENhgXLPA-geVpEYD1YFCNatrUHczRJHMwCwV8ZlMuQI3xQF9z0gsLNlchewvy5PtLOE7xnFyka429Wisz3pZ-lrTTRe3QnTuVWSfjWBwtrXbx0IDkGnNBet18HDQzJHnpKwPPtsc0_QGagNPJhOyNQ6Pp7wVrnWGM0e7U-hVb7ydLPdVaRYrmU26re4OjbGdXB6PBZIzqQOFgB-LaQ_Yyt-SmxOFsEOltBNjY3kFmTi7QQkW8mAM-ysiy7cGqlCo8LnRnRjlJhy3SHoLSA9WhPlnYACZt0gL5flxtPUon1m__aHcZE6FjCq8wQ52DfYA --src-creds wkulhane+jenkins:77TU1T1RUSI9I1VCCR8M6Y7SF5MLJZTQBY4PFCA74TSQRW7EM2NXG8IFDQ2F1K7Z docker://quay.rhte.example.opentlc.com/wkulhane/rhte-app:1.4 docker://docker-registry-default.apps.rhte-cloud2.example.opentlc.com/wkulhane-rhte-app/rhte-app:1.4
Getting image source signatures
Copying blob sha256:428a9ca37f0e50a5d54a0540773f2c7d402b27ed1264cfef3b7b8d87ee102ec8
 71.45 MB / ?
 71.45 MB / ?  6s
Copying blob sha256:8115a58d83bdb789829910c62fc48223d5204954608a08f612f06bdd162677ab

 0 B / ?
 1.27 KB / ?
 1.27 KB / ?  0s
Copying blob sha256:339bc4cdfd0f5fb6f1efb3efc66de4968af86634e1d852bfdb68a74ac66d5a39

 0 B / ?
 1.83 MB / ?
 5.33 MB / ?
 7.30 MB / ?
 7.30 MB / ?  1s
Copying blob sha256:a1a7c4c6afd5e6c93096132070c2f2307a57131c0ac672ac70a684475d316fe9
 79.81 MB / ?
 79.81 MB / ?  7s
Copying blob sha256:a41ced1af98ed6be4aff96df12e8c7a53f57961d40a69e68fdd2a59485a25795
 7.96 MB / ?
 7.96 MB / ?  1s
Copying blob sha256:4d2c6556481974e9ac9519f75ebdc77b9d3359f66cfceec8774d1512eb8df1da

 0 B / ?
 1.96 MB / ?
 3.97 MB / ?
 3.97 MB / ?  0s
Writing manifest to image destination
Storing signatures
[Pipeline] }

[...]
----



== Enhance the Jenkins Pipeline to deploy the image on the Production Clusters

The last step in our pipeline is to actually deploy this image using the deployment configurations that you created earlier. Again for simplicity you are doing a very simple "oc rollout latest" while in a proper production environment you would most likely execute a Blue/Green deployment to avoid any downtime - and have an easy way to roll back to a previously working version in case anything went wrong.

. Open your Pipeline configuration again by clicking *Configure* (on the left) on the Pipeline Item.
. Add this code to the pipeline between the markers `// Deploy Images BEGIN` and `// Deploy Images END`:
+
[source,groovy]
----
    stage("Deploy Production Applications on Cluster 1") {
      steps {
        script {
        // Update image for Deployment Config on Cluster 1
          openshift.withCluster("https://master.${cluster_base}", "${cluster_TOKEN}") {

            // Use the production project
            openshift.withProject("${PREFIX}-rhte-app") {
              // Update the image in the deployment config to point to the image you just copied
              // from Quay.
              openshift.set("image", "dc/rhte-app", "rhte-app=docker-registry.default.svc:5000/${PREFIX}-rhte-app/rhte-app:${prodTag}")

              // Set the environment variable IMAGE_TAG to the current image tag. The Node.JS
              // application displays this environment variable.
              openshift.set("env", "dc/rhte-app", "IMAGE_TAG=${prodTag}")

              // Redeploy the application.
              openshift.selector("dc", "rhte-app").rollout().latest();
            }
          }
        }
      }
    }
    stage("Deploy Production Applications on Cluster 2") {
      steps {
        script {
        // Update image for Deployment Config (same as for cluster 1)
          openshift.withCluster("insecure://master.${cluster2_base}", "${CLUSTER2_TOKEN}") {
            openshift.withProject("${PREFIX}-rhte-app") {
              openshift.set("image", "dc/rhte-app", "rhte-app=docker-registry.default.svc:5000/${PREFIX}-rhte-app/rhte-app:${prodTag}")
              openshift.set("env", "dc/rhte-app", "IMAGE_TAG=${prodTag}")
              openshift.selector("dc", "rhte-app").rollout().latest();
            }
          }
        }
      }
    }
----
+
. Save the updated pipeline and build it again.
. Once your build completes successfully your application will be running on both clusters.
. You will see in the build logs that the application deployed successfully on both clusters.
+
.Sample Output
[source,text]
----
[...]

[Pipeline] stage
[Pipeline] { (Deploy Production Applications on Cluster 1)
[Pipeline] script
[Pipeline] {
[Pipeline] _OcContextInit
[Pipeline] _OcContextInit
[Pipeline] _OcAction
[Pipeline] _OcAction
[Pipeline] _OcAction
[rollout:latest:deploymentconfig/rhte-app] deploymentconfig "rhte-app" rolled out
[rollout:latest:deploymentconfig/rhte-app]
[Pipeline] }
[Pipeline] // script
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Deploy Production Applications on Cluster 2)
[Pipeline] script
[Pipeline] {
[Pipeline] _OcContextInit
[Pipeline] _OcContextInit
[Pipeline] _OcAction
[Pipeline] _OcAction
[Pipeline] _OcAction
[rollout:latest:deploymentconfig/rhte-app] deploymentconfig "rhte-app" rolled out
[rollout:latest:deploymentconfig/rhte-app]
[Pipeline] }
[Pipeline] // script
[Pipeline] }
[Pipeline] // stage
[Pipeline] }
[Pipeline] // container
[Pipeline] }
[Pipeline] // node
[Pipeline] }
[Pipeline] // podTemplate
[Pipeline] End of Pipeline
Finished: SUCCESS
----


=== Verify Application on both Clusters

Now that your application has been deployed to both clusters it is time to verify the applications.

. First Verify the application on Cluster 1.
+
In your web browser open the URL to your application (replacing `xyz123` with your shortname):
+
====
http://rhte-app-xyz123-rhte-app.apps.{cluster_base}
====
+
You should see a web site that shows the Cluster Name ("Cluster 1") as well as the tag of your image (which is built using the build number from Jenkins).

. Second verify the application on Cluster 2.
+
In your web browser open the URL to your application (replacing `xyz123` with your shortname):
+
====
http://rhte-app-xyz123-rhte-app.apps.{cluster2_base}
====
+
You should see a web site that shows the Cluster Name ("Cluster 2") as well as the tag of your image. The Image tag should be the same as in the first cluster.
+
. Third verify that the load balancer that is in front of our two clusters routes to your application correctly.
+
In your web browser open the URL to your application (replacing `xyz123` with your shortname):
+
====
http://xyz123.apps.rhte.example.opentlc.com
====
+
You will be randomly routed to one of your two clusters. Note that in a web browser the session affinity will prevent you from observing any load balancing.
+
. To observe load balancing open a terminal window and type the following command (make sure to set the *GUID* variable to your shortname if it not still set):
+
[source,bash]
----
export GUID=xyz123
for i in {1..20}
do
  curl http://${GUID}.apps.rhte.example.opentlc.com/cluster
done
----
+
.Sample Output
[source,text]
----
Cluster 1
Cluster 2
Cluster 1
Cluster 2
[...]
----
+
You will see that the load balancer does a simple round robin load balancing between the application on cluster 1 and cluster 2.

== Cleanup

Now that you have finished this lab, please take a moment to clean up the environment for the next set of students.

. Log into the OpenShift Cluster 1 (https://{cluster_master}).
+
[source,bash]
----
oc login -u <OpenTLC User Name> -p <OpenTLC Password> <CLUSTER_URL>
----
+
. Delete the Jenkins Project on Cluster 1
+
[source,bash]
----
oc delete project ${GUID}-jenkins
----
+
. Delete the Development Project on Cluster 1
+
[source,bash]
----
oc delete project ${GUID}-rhte-app-dev
----
+
. Delete the Production Project on Cluster 1
+
[source,bash]
----
oc delete project ${GUID}-rhte-app
----
+
. Log into the OpenShift Cluster 2 (https://{cluster2_master}).
+
[source,bash]
----
oc login -u <OpenTLC User Name> -p <OpenTLC Password> <CLUSTER_URL>
----
+
. Delete the Production Project on Cluster 2
+
[source,bash]
----
oc delete project ${GUID}-rhte-app
----

Congratulations! You finished this lab.
